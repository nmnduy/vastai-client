---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: vastai-db-pvc # Name of the PVC
spec:
  accessModes:
    - ReadWriteOnce # Suitable for single-node access, common for block storage
  resources:
    requests:
      storage: 200Gi # Request 200Gi of storage
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vastai-client-server
  labels:
    app: vastai-client-server
spec:
  replicas: 1 # Keep replicas at 1 for SQLite writer safety
  selector:
    matchLabels:
      app: vastai-client-server
  template:
    metadata:
      labels:
        app: vastai-client-server
    spec:
      volumes: # Define the volume using the PersistentVolumeClaim
      - name: db-storage
        persistentVolumeClaim:
          claimName: vastai-db-pvc # Reference the PVC created above
      containers:
      - name: server # Main application container
        image: 073692673157.dkr.ecr.ap-southeast-1.amazonaws.com/vastai-client-server:v0.0.2
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 50051
          name: grpc
          protocol: TCP
        volumeMounts: # Mount the PVC volume
        - name: db-storage
          mountPath: /data # Mount the PVC volume at /data
        env:
          # Database configuration - SQLite file path on the PVC
          - name: DATABASE_URL
            value: "/data/vastai_status.db"
        resources:
          requests:
            cpu: "100m"
            memory: "128Mi"
          limits:
            cpu: "500m"
            memory: "512Mi"
        livenessProbe:
          tcpSocket:
            port: grpc
          # Delays might still be relevant if Litestream restore takes time on first start
          initialDelaySeconds: 25
          periodSeconds: 20
          timeoutSeconds: 5
          failureThreshold: 3
        readinessProbe:
          tcpSocket:
            port: grpc
          initialDelaySeconds: 15
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3

---

apiVersion: v1
kind: ConfigMap
metadata:
  name: litestream-config
data:
  litestream.yml: |
    # Global S3 credentials (using environment variables from K8s secret)
    access-key-id: ${LITESTREAM_ACCESS_KEY_ID}
    secret-access-key: ${LITESTREAM_SECRET_ACCESS_KEY}

    dbs:
      - path: /data/vastai_status.db # Path where the database is mounted inside the container
        replicas:
          - type: s3                   # Use 's3' type for S3-compatible storage
            bucket: vastai-client      # Your R2 bucket name
            path: vastai_client.db     # Path within the bucket to store the replica
            endpoint: https://d1d69208d12bdf919319004fa00204c5.r2.cloudflarestorage.com # R2 endpoint URL

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: litestream-replicator
  labels:
    app: litestream-replicator
spec:
  replicas: 1
  selector:
    matchLabels:
      app: litestream-replicator
  template:
    metadata:
      labels:
        app: litestream-replicator
    spec:
      volumes:
      # Define the volume using the same PersistentVolumeClaim
      - name: db-storage
        persistentVolumeClaim:
          claimName: vastai-db-pvc # Reference the PVC
      # Add volume for the ConfigMap
      - name: config-volume
        configMap:
          name: litestream-config # Reference the ConfigMap created above
      containers:
      - name: litestream
        image: litestream/litestream:latest
        args:
          # Use the config file instead of command line args for db/replica
          - "replicate"
          - "-config"
          - "/etc/litestream.yml" # Path where the config file is mounted
        env:
          # Load R2 credentials from the Kubernetes secret
          - name: LITESTREAM_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                name: r2-credentials
                key: LITESTREAM_ACCESS_KEY_ID
          - name: LITESTREAM_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                name: r2-credentials
                key: LITESTREAM_SECRET_ACCESS_KEY
          # No longer need LITESTREAM_ENDPOINT_URL env var, it's in the config file
        volumeMounts:
          - name: db-storage # Mount the PVC volume
            mountPath: /data # Mount path must match the 'path' in litestream.yml
          # Mount the config file into the container
          - name: config-volume
            mountPath: /etc/litestream.yml # Default config file path
            subPath: litestream.yml       # Mount only the litestream.yml key
            readOnly: true                # Config file doesn't need to be writable
        resources:
          requests:
            cpu: "50m"
            memory: "64Mi"
          limits:
            cpu: "100m"
            memory: "128Mi"

---
# Service to expose the gRPC Server within the cluster
apiVersion: v1
kind: Service
metadata:
  name: vastai-client-server-service
  labels:
    app: vastai-client-server
spec:
  selector:
    app: vastai-client-server
  ports:
    - protocol: TCP
      port: 50051
      targetPort: grpc # Points to the grpc port defined in the 'server' container
      name: grpc
  type: ClusterIP
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: sqlite-inspector
  labels:
    app: sqlite-inspector
spec:
  replicas: 1
  selector:
    matchLabels:
      app: sqlite-inspector
  template:
    metadata:
      labels:
        app: sqlite-inspector
    spec:
      volumes:
        - name: db-storage
          persistentVolumeClaim:
            claimName: vastai-db-pvc
      containers:
      - name: inspector
        image: ubuntu:latest
        command: ["/bin/sh", "-c"]
        args:
          - apt-get update && apt-get install -y sqlite3 && echo 'SQLite3 installed. Sleeping indefinitely...' && sleep infinity
        volumeMounts:
          - name: db-storage
            mountPath: /data
        resources:
          requests:
            cpu: "50m"
            memory: "64Mi"
          limits:
            cpu: "100m"
            memory: "128Mi"
